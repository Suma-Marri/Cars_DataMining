---
title: "Assignment - Dimension Reduction"
output:
  pdf_document: default
  html_notebook: default
---
-------------------------------------------------------------------------------


## Loading the Data
```{r}
Cars_df <- read.csv(
  file = "C:/Users/Suma Marri/Documents/GitHub/USCars/USA_cars_datasets.csv",
  colClasses = "character"
  )
```

This US Cars Dataset data was scraped from AUCTION EXPORT.com. 
The dataset includes information about 28 brands of clean and used vehicles for sale in US.

## Organized Dataframe and Dummy Coding
```{r}
price <- as.double(Cars_df$price)
mileage <- as.double(Cars_df$mileage)
brand <- as.character(Cars_df$brand)
year <- as.character(Cars_df$year)

d <- data.frame(brand, price, mileage, year)
```

#### Treating brand, color, model, and brand as categorical/ordinal predictors (dummy coding)
```{r}
library(fastDummies)
d_dummy <- dummy_cols(d, select_columns = 'year',
                      remove_selected_columns = TRUE)
```
The result after dummy coding produces a large number of predictor columns.


#### Levels of the target variable 'brand'
```{r}
table(d_dummy$brand)
```

## PCA 

#### Creating a color vector corresponding to the target variable
```{r}
brand_color <- viridis::viridis(28)
names(brand_color) <- sort(unique(d_dummy$brand))
brand_color <- brand_color[d_dummy$brand]
```

#### Principal Component Analysis (RANK 2 PCA) - Visualizations (Biplot and Screeplot)
```{r}
D <- d_dummy[,colnames(d_dummy)!="brand"]
prcomp_D <- prcomp(
  x=D,
  center=TRUE,
  scale.=TRUE,
  rank. = 2
)

round(summary(prcomp_D)$importance,2)
biplot(prcomp_D)
factoextra::fviz_eig(prcomp_D)
```

#### Lets look at how well theese 2 predictors perform:
```{r}
pairs(prcomp_D$x[,1:2], col=brand_color, pch=19)

```

#### Looking at the scatterplots above, we can be confident in performing rank two PCA analysis since the scatter plots seem to be a reflection of each other.
```{r}
plot(prcomp_D$x[,1:2],col=brand_color,pch=19)

```
In the plot above, we can see separation between the colors. Although, most of the data points in the scatter plot are close together but a slight separation can still be seen. Only a few predictors could be distinguished looking at this plot, since most of the data points are clumped together.

#### Using rotation matrix to interpret the Principal Components

##### PC1
```{r}
v <- sort(prcomp_D$rotation[,"PC1"])
v <- v[abs(v) > 0.15]
M <- matrix(v)
rownames(M) <- names(v)
M
par(las=3)
barplot(prcomp_D$rotation[,"PC1"])
```

##### PC2
```{r}
v <- sort(prcomp_D$rotation[,"PC2"])
v <- v[abs(v) > 0.15]
M <- matrix(v)
rownames(M) <- names(v)
M
par(las=3)
barplot(prcomp_D$rotation[,"PC2"])
```
From the rotation matrix interpretation, we can interpret for PC1 that price, mileage, and year_2019 are good predictors of brand. Whereas for PC2, year_2017, year_2018, and year_2019 are good predictors of brand. The predictors are interpreted on being strong predictors based on the correlation values on the higher ends (negative or positive). If we perform feature engineering, maybe price and year_2019 can form a better predictor, or price and mileage can be formulated in to a solid predictor.

#### Scatterplot for the two components
```{r}
plot(prcomp_D$x[,1:2],
     col = brand_color,
     pch = as.character(brand),
     main = "Final Scatterplot of PCA - For Comparison")
```

# t-SNE
The second technique in this problem for dimension reduction is t-Distributed Stochastic Neighbor Embedding (t-distribution).

#### t-Distributed Stochastic Neighbour Embedding Analysis (RANK 2)
```{r}
set.seed(823)
Rtsne_d <- Rtsne::Rtsne(
  X = D,
  check_duplicates = FALSE
)
plot(
  Rtsne_d$Y,
  col = brand_color,
  pch = as.character(brand),
  main = "Scatter plot of T-SNE - 2 Dimensions"
)
```


#### Lets adjust the hyperparameters to investigate performance improvement
```{r}
set.seed(823)
Rtsne2_d <- Rtsne::Rtsne(
  X = D,
  check_duplicates = FALSE,
  dims = 3,
  PCA = FALSE,
  max_iter = 2000,
  perplexity = 50
)
plot(
  Rtsne2_d$Y,
  col = brand_color,
  pch = as.character(brand),
  main = "Scatter plot of T-SNE - 2 Dimensions"
)

```
We can see from both the iterations above for the scatter plots of T-SNE, the data points are scattered across the plot. The data points are still close together in both the plots compared to PCA, where some sort of grouping/distinction was seen in the data points. 
After the adjustment of hyperparameters for t-SNE, the data points still seem to be clumped together, however now we can see the green data points "n" are grouping together.


#### Lets check if the model converged
```{r}
plot(Rtsne_d$itercosts)
```

# Nonnegative matrix factorization
The last technique for dimension reduction in this problem is nonnegative matrix factorization.


#### Running NMF
```{r}
# Scale the dataset
library(dplyr)
library(scales)
#install.packages("BiocManager")
#BiocManager::install("Biobase")

scaled_D <- D%>%mutate_if(is.double, rescale, to = c(0,1))
scaled_D
#Check if scaled_D has any negative numbers
range(scaled_D)

nmf_D <- NMF::nmf(scaled_D, 2)
nmf_D
```

#### Using the NMF basis function to retrieve a new dataset with reduced columns
```{r}
basis_D <- NMF::basis(
  object = nmf_D
)

coef_D <- NMF::coef(
  object = nmf_D
)

dim(basis_D)
dim(coef_D)

colnames(basis_D) <- c(
  "topic_1","topic_2"
)
rownames(coef_D) <- c(
  "topic_1","topic_2"
)
round(head(basis_D),3)
round(coef_D,3)
```

#### Plotting the two basis vectors
```{r}
plot(basis_D,
     col = brand_color,
     pch = as.character(brand),
     main = "Scatterplot for NMF - two basis vectors")
```
Int the scatter plot,there is some separation between the data points, but, some of the data still seems to be clumped together. PCA seemed to do a better job of grouping the p points together and separate from the rest of the data points. t-SNE had data points all scattered throughout the plot even after adjusting the parameters for the Rtsne function. Furthermore, although PCA had the data points clumped together, it seemed to have chunks of data points separate from each other. This can be interpreted from the analysis of predicting a car brand using price, mileage, and year. The analysis also shows that different brands of cars can be in different categories (High-mileage, High-price, and Latest Year). The number of brands of cars are unequal in this dataset, which makes this problem more complicated and our analysis inaccurate. A better analysis could be performed by predicting the brand of a car using price, mileage, and the year if there were a close equal number of car brands in the data.
